{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iFXBI_JSjTA"
      },
      "source": [
        "# **DIA1. Introducci√≥n a Apache Airflow**\n",
        "## Ejercicio: Crear tu primer DAG funcional\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è IMPORTANTE: Compatibilidad con Windows\n",
        "\n",
        "**Apache Airflow NO es compatible de forma nativa con Windows.** Airflow est√° dise√±ado para sistemas POSIX (Linux/macOS).\n",
        "\n",
        "#### Opciones para usar Airflow en Windows:\n",
        "\n",
        "1. **Docker (RECOMENDADO)** - La forma m√°s sencilla y confiable\n",
        "2. **WSL2 (Windows Subsystem for Linux)** - Ejecutar Linux dentro de Windows\n",
        "3. **M√°quina Virtual** - Instalar Linux en una VM\n",
        "\n",
        "Este notebook ha sido actualizado para mostrar ambas opciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbleh3qKStyd"
      },
      "source": [
        "### 1.- Instalaci√≥n b√°sica de Airflow\n",
        "\n",
        "#### Opci√≥n A: Usando Docker (RECOMENDADO para Windows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPCI√ìN A: Instalaci√≥n con Docker\n",
        "# \n",
        "# Paso 1: Instalar Docker Desktop para Windows desde:\n",
        "# https://www.docker.com/products/docker-desktop\n",
        "#\n",
        "# Paso 2: Crear un directorio para Airflow\n",
        "# mkdir airflow-docker\n",
        "# cd airflow-docker\n",
        "#\n",
        "# Paso 3: Descargar el archivo docker-compose.yaml oficial\n",
        "# curl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml'\n",
        "#\n",
        "# Paso 4: Crear directorios necesarios\n",
        "# mkdir -p ./dags ./logs ./plugins ./config\n",
        "#\n",
        "# Paso 5: Configurar el usuario (en PowerShell)\n",
        "# echo \"AIRFLOW_UID=50000\" > .env\n",
        "#\n",
        "# Paso 6: Inicializar la base de datos\n",
        "# docker-compose up airflow-init\n",
        "#\n",
        "# Paso 7: Iniciar todos los servicios\n",
        "# docker-compose up\n",
        "#\n",
        "# Paso 8: Acceder a la interfaz web\n",
        "# http://localhost:8080\n",
        "# Usuario: airflow\n",
        "# Contrase√±a: airflow\n",
        "\n",
        "print(\"Para usar Airflow en Windows, sigue los pasos comentados arriba usando Docker.\")\n",
        "print(\"Docker es la forma m√°s confiable de ejecutar Airflow en Windows.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Opci√≥n B: Usando WSL2 (Windows Subsystem for Linux)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPCI√ìN B: Instalaci√≥n en WSL2\n",
        "#\n",
        "# Paso 1: Instalar WSL2 (ejecutar en PowerShell como administrador)\n",
        "# wsl --install\n",
        "#\n",
        "# Paso 2: Reiniciar el sistema\n",
        "#\n",
        "# Paso 3: Abrir WSL (Ubuntu) y ejecutar los siguientes comandos:\n",
        "#\n",
        "# # Actualizar el sistema\n",
        "# sudo apt update && sudo apt upgrade -y\n",
        "#\n",
        "# # Instalar Python y pip\n",
        "# sudo apt install python3 python3-pip python3-venv -y\n",
        "#\n",
        "# # Crear entorno virtual\n",
        "# python3 -m venv airflow_env\n",
        "# source airflow_env/bin/activate\n",
        "#\n",
        "# # Instalar Airflow\n",
        "# pip install apache-airflow\n",
        "#\n",
        "# # Inicializar base de datos\n",
        "# airflow db init\n",
        "#\n",
        "# # Crear usuario admin\n",
        "# airflow users create \\\n",
        "#     --username admin \\\n",
        "#     --firstname Admin \\\n",
        "#     --lastname User \\\n",
        "#     --role Admin \\\n",
        "#     --email admin@example.com \\\n",
        "#     --password admin\n",
        "\n",
        "print(\"Para usar Airflow con WSL2, sigue los pasos comentados arriba.\")\n",
        "print(\"WSL2 te permite ejecutar Linux dentro de Windows de forma nativa.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo6VyPIyTGQu"
      },
      "source": [
        "### 2.- Crear primer DAG\n",
        "\n",
        "Este c√≥digo debe guardarse en un archivo `.py` en la carpeta `dags/` de tu instalaci√≥n de Airflow.\n",
        "\n",
        "**Ubicaci√≥n del archivo:**\n",
        "- Docker: `./dags/mi_primer_dag.py`\n",
        "- WSL2: `~/airflow/dags/mi_primer_dag.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contenido del archivo: dags/mi_primer_dag.py\n",
        "# NOTA: Este c√≥digo NO se ejecuta directamente en Jupyter.\n",
        "# Debe guardarse como archivo .py en la carpeta dags/ de Airflow.\n",
        "\n",
        "codigo_dag = '''\n",
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def saludar():\n",
        "    \"\"\"Funci√≥n que imprime un saludo.\"\"\"\n",
        "    print(\"¬°Hola desde Airflow!\")\n",
        "    return \"Saludo completado\"\n",
        "\n",
        "# Argumentos por defecto para el DAG\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Definir DAG\n",
        "with DAG(\n",
        "    'saludo_diario',\n",
        "    default_args=default_args,\n",
        "    description='DAG que saluda cada d√≠a',\n",
        "    schedule_interval=timedelta(days=1),  # Ejecutar diariamente\n",
        "    start_date=datetime(2024, 1, 1),\n",
        "    catchup=False,  # No ejecutar ejecuciones pasadas\n",
        "    tags=['ejemplo', 'saludo'],\n",
        ") as dag:\n",
        "\n",
        "    # Tarea 1: Comando bash\n",
        "    tarea_bash = BashOperator(\n",
        "        task_id='tarea_bash',\n",
        "        bash_command='echo \"Ejecutando tarea bash a las $(date)\"',\n",
        "    )\n",
        "\n",
        "    # Tarea 2: Funci√≥n Python\n",
        "    tarea_python = PythonOperator(\n",
        "        task_id='tarea_python',\n",
        "        python_callable=saludar,\n",
        "    )\n",
        "\n",
        "    # Tarea 3: Esperar (simular procesamiento)\n",
        "    tarea_esperar = BashOperator(\n",
        "        task_id='tarea_esperar',\n",
        "        bash_command='sleep 5',\n",
        "    )\n",
        "\n",
        "    # Definir orden de ejecuci√≥n\n",
        "    tarea_bash >> tarea_python >> tarea_esperar\n",
        "'''\n",
        "\n",
        "# Guardar el c√≥digo en un archivo\n",
        "print(\"C√≥digo del DAG:\")\n",
        "print(codigo_dag)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INSTRUCCIONES:\")\n",
        "print(\"1. Copia el c√≥digo de arriba\")\n",
        "print(\"2. Gu√°rdalo en: dags/mi_primer_dag.py\")\n",
        "print(\"3. Airflow detectar√° autom√°ticamente el archivo\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.- Ejecutar y monitorear\n",
        "\n",
        "Estos comandos se ejecutan en la terminal, NO en Jupyter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMANDOS PARA EJECUTAR EN TERMINAL (no en Jupyter)\n",
        "\n",
        "comandos = '''\n",
        "# === OPCI√ìN A: Si usas Docker ===\n",
        "# Los servicios ya est√°n corriendo con docker-compose up\n",
        "# Solo necesitas acceder a http://localhost:8080\n",
        "\n",
        "# === OPCI√ìN B: Si usas WSL2 ===\n",
        "# Terminal 1: Iniciar scheduler\n",
        "airflow scheduler\n",
        "\n",
        "# Terminal 2: Iniciar webserver\n",
        "airflow webserver --port 8080\n",
        "\n",
        "# Terminal 3: Activar y ejecutar DAG\n",
        "airflow dags unpause saludo_diario\n",
        "airflow dags trigger saludo_diario\n",
        "\n",
        "# Ver lista de DAGs\n",
        "airflow dags list\n",
        "\n",
        "# Ver tareas de un DAG\n",
        "airflow tasks list saludo_diario\n",
        "\n",
        "# Ver estado de ejecuciones\n",
        "airflow dags list-runs -d saludo_diario\n",
        "'''\n",
        "\n",
        "print(\"COMANDOS PARA EJECUTAR EN TERMINAL:\")\n",
        "print(\"=\"*60)\n",
        "print(comandos)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.- Ver resultados\n",
        "\n",
        "La interfaz web de Airflow es la forma principal de monitorear tus DAGs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ACCEDER A LA INTERFAZ WEB\n",
        "\n",
        "instrucciones = '''\n",
        "1. Abrir navegador web\n",
        "2. Ir a: http://localhost:8080\n",
        "3. Iniciar sesi√≥n:\n",
        "   - Usuario: airflow (o admin si usas WSL2)\n",
        "   - Contrase√±a: airflow (o admin si usas WSL2)\n",
        "\n",
        "4. Navegar por la interfaz:\n",
        "   - DAGs: Lista de todos los DAGs disponibles\n",
        "   - Graph View: Visualizaci√≥n del flujo de tareas\n",
        "   - Tree View: Historial de ejecuciones\n",
        "   - Gantt: Diagrama de Gantt de las tareas\n",
        "   - Code: Ver el c√≥digo del DAG\n",
        "   - Logs: Ver los logs de cada tarea\n",
        "\n",
        "5. Ejecutar manualmente:\n",
        "   - Click en el DAG \"saludo_diario\"\n",
        "   - Click en el bot√≥n \"Play\" (‚ñ∂) en la esquina superior derecha\n",
        "   - Seleccionar \"Trigger DAG\"\n",
        "\n",
        "6. Ver logs:\n",
        "   - Click en una tarea en el Graph View\n",
        "   - Click en \"Log\"\n",
        "   - Ver la salida de la ejecuci√≥n\n",
        "'''\n",
        "\n",
        "print(\"C√ìMO USAR LA INTERFAZ WEB DE AIRFLOW:\")\n",
        "print(\"=\"*60)\n",
        "print(instrucciones)\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.- Conceptos clave de Airflow\n",
        "\n",
        "Entender estos conceptos es fundamental para trabajar con Airflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conceptos = '''\n",
        "üìö CONCEPTOS CLAVE:\n",
        "\n",
        "1. DAG (Directed Acyclic Graph):\n",
        "   - Grafo dirigido ac√≠clico que define el flujo de trabajo\n",
        "   - Contiene tareas y sus dependencias\n",
        "   - No puede tener ciclos (A -> B -> C -> A ‚ùå)\n",
        "\n",
        "2. Task (Tarea):\n",
        "   - Unidad b√°sica de trabajo en Airflow\n",
        "   - Puede ser: BashOperator, PythonOperator, etc.\n",
        "   - Tiene un estado: success, failed, running, etc.\n",
        "\n",
        "3. Operator (Operador):\n",
        "   - Plantilla para crear tareas\n",
        "   - Tipos comunes:\n",
        "     * BashOperator: Ejecuta comandos bash\n",
        "     * PythonOperator: Ejecuta funciones Python\n",
        "     * EmailOperator: Env√≠a emails\n",
        "     * SQLOperator: Ejecuta queries SQL\n",
        "\n",
        "4. Schedule Interval:\n",
        "   - Frecuencia de ejecuci√≥n del DAG\n",
        "   - Puede ser: cron expression o timedelta\n",
        "   - Ejemplos:\n",
        "     * '@daily': Una vez al d√≠a\n",
        "     * '@hourly': Una vez por hora\n",
        "     * '0 0 * * *': Cron expression (medianoche)\n",
        "\n",
        "5. Execution Date:\n",
        "   - Fecha l√≥gica de ejecuci√≥n (no la fecha real)\n",
        "   - Importante para procesar datos hist√≥ricos\n",
        "\n",
        "6. Catchup:\n",
        "   - Si True: Ejecuta todas las ejecuciones pasadas\n",
        "   - Si False: Solo ejecuta desde ahora en adelante\n",
        "\n",
        "7. Dependencies (Dependencias):\n",
        "   - Definen el orden de ejecuci√≥n\n",
        "   - Sintaxis: task1 >> task2 >> task3\n",
        "   - O: task1.set_downstream(task2)\n",
        "'''\n",
        "\n",
        "print(conceptos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.- Troubleshooting (Soluci√≥n de problemas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "troubleshooting = '''\n",
        "üîß PROBLEMAS COMUNES Y SOLUCIONES:\n",
        "\n",
        "1. \"ModuleNotFoundError: No module named 'airflow'\"\n",
        "   Soluci√≥n:\n",
        "   - Aseg√∫rate de estar en el entorno virtual correcto\n",
        "   - Reinstala: pip install apache-airflow\n",
        "\n",
        "2. \"AttributeError: module 'os' has no attribute 'register_at_fork'\"\n",
        "   Soluci√≥n:\n",
        "   - Est√°s en Windows. Usa Docker o WSL2\n",
        "   - Airflow NO funciona nativamente en Windows\n",
        "\n",
        "3. \"DAG no aparece en la interfaz web\"\n",
        "   Soluci√≥n:\n",
        "   - Verifica que el archivo est√© en la carpeta dags/\n",
        "   - Revisa los logs: airflow dags list-import-errors\n",
        "   - Espera ~30 segundos (Airflow escanea cada 30s)\n",
        "\n",
        "4. \"Puerto 8080 ya est√° en uso\"\n",
        "   Soluci√≥n:\n",
        "   - Usa otro puerto: airflow webserver --port 8081\n",
        "   - O det√©n el proceso: lsof -ti:8080 | xargs kill -9\n",
        "\n",
        "5. \"Tarea falla sin raz√≥n aparente\"\n",
        "   Soluci√≥n:\n",
        "   - Revisa los logs en la interfaz web\n",
        "   - Verifica las dependencias de Python\n",
        "   - Prueba la funci√≥n fuera de Airflow primero\n",
        "\n",
        "6. \"Base de datos bloqueada (SQLite)\"\n",
        "   Soluci√≥n:\n",
        "   - SQLite no es para producci√≥n\n",
        "   - Usa PostgreSQL o MySQL en producci√≥n\n",
        "   - Para desarrollo: reinicia scheduler y webserver\n",
        "'''\n",
        "\n",
        "print(troubleshooting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.- Pr√≥ximos pasos\n",
        "\n",
        "Una vez que tengas tu primer DAG funcionando, puedes explorar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proximos_pasos = '''\n",
        "üöÄ PR√ìXIMOS PASOS:\n",
        "\n",
        "1. Explorar m√°s operadores:\n",
        "   - EmailOperator: Enviar notificaciones\n",
        "   - PostgresOperator: Ejecutar queries SQL\n",
        "   - HttpOperator: Hacer peticiones HTTP\n",
        "   - DockerOperator: Ejecutar contenedores\n",
        "\n",
        "2. Trabajar con XComs:\n",
        "   - Compartir datos entre tareas\n",
        "   - Pasar resultados de una tarea a otra\n",
        "\n",
        "3. Usar Variables y Connections:\n",
        "   - Almacenar configuraciones\n",
        "   - Conectar a bases de datos externas\n",
        "\n",
        "4. Implementar branching:\n",
        "   - BranchPythonOperator\n",
        "   - Ejecutar tareas condicionalmente\n",
        "\n",
        "5. Configurar alertas:\n",
        "   - Email en caso de fallo\n",
        "   - Integraci√≥n con Slack\n",
        "\n",
        "6. Optimizar performance:\n",
        "   - Paralelizaci√≥n de tareas\n",
        "   - Pools y prioridades\n",
        "\n",
        "7. Desplegar en producci√≥n:\n",
        "   - Usar PostgreSQL/MySQL\n",
        "   - Configurar CeleryExecutor\n",
        "   - Implementar monitoring\n",
        "'''\n",
        "\n",
        "print(proximos_pasos)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
